{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "os.environ['OPENAI_API_KEY'] = open(os.path.expanduser('~/.openai-bloomberg-project-key.txt')).read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ArticleOutput(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    url: str\n",
    "    publication_date: str\n",
    "    content: str\n",
    "\n",
    "class WikipediaArticleOutput(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "    url: str\n",
    "    publication_date: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "CLEAN_RAW_ARTICLE_TEXT_PROMPT = \"\"\"You are a helpful assistant. \n",
    "Here is raw text from a news webpage I copied from the internet.\n",
    "Please extract the article title, author, publication date, and all the article content.\n",
    "Remove any text that is not part of the article content.\n",
    "Copy the article content exactly as is, do not summarize anything.\n",
    "\n",
    "Return in a JSON format with the following fields: title, author, url, publication_date, content.\n",
    "All the fields should be strings.\n",
    "\n",
    "<article>\n",
    "{raw_article_text}\n",
    "</article>\n",
    "\"\"\"\n",
    "\n",
    "CLEAN_WIKIPEDIA_ARTICLE_TEXT_PROMPT = \"\"\"You are a helpful assistant. \n",
    "Here is raw text from a Wikipedia article I copied from the internet.\n",
    "Please extract the article title, author, publication date, content and citations.\n",
    "In the content field, keep ALL citation markers (e.g. [1], [2], [3], etc.), copy them exactly as they are. \n",
    "Remove any text that is not part of the article content.\n",
    "\n",
    "Return in a JSON format with the following fields: title, author, url, publication_date, content.\n",
    "\n",
    "<article>\n",
    "{raw_article_text}\n",
    "</article>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_openai(prompt: str, response_format: BaseModel):\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        response_format=response_format\n",
    "    )    \n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e27a0be2cfb45e4923a666414a30a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob \n",
    "from tqdm.auto import tqdm\n",
    "files = glob.glob('../data/*/*.txt')\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    _, _, issue, filename = file.split('/')\n",
    "    raw_article_text = open(file).read()\n",
    "\n",
    "    if filename == 'wikipedia.txt':\n",
    "        article_prompt = CLEAN_WIKIPEDIA_ARTICLE_TEXT_PROMPT.format(raw_article_text=raw_article_text)\n",
    "        response_format = WikipediaArticleOutput\n",
    "    else:\n",
    "        article_prompt = CLEAN_RAW_ARTICLE_TEXT_PROMPT.format(raw_article_text=raw_article_text)\n",
    "        response_format = ArticleOutput\n",
    "\n",
    "    article = prompt_openai(article_prompt, response_format)\n",
    "    article_dict = dict(article)\n",
    "    article_dict['issue'] = issue\n",
    "    article_dict['filename'] = filename\n",
    "    all_articles.append(article_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_df = pd.DataFrame(all_articles)\n",
    "all_articles_df.to_csv('../data/all-articles-parsed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_citations(html: str):\n",
    "    \"\"\"\n",
    "    Parse a Wikipedia <div class=\"reflist\"> block and return a list of dicts:\n",
    "      - citation_number: int\n",
    "      - text: str\n",
    "      - retrieval_date: str or None\n",
    "      - url: str or None\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # select all list items in the references list\n",
    "    items = soup.select('div.reflist ol.references > li')\n",
    "    results = []\n",
    "    for idx, li in enumerate(items, start=1):\n",
    "        # 1) full citation text\n",
    "        ref_span = li.find('span', class_='reference-text')\n",
    "        text = ref_span.get_text(separator=' ', strip=True) if ref_span else ''\n",
    "\n",
    "        # 2) retrieval date, if present\n",
    "        access = li.find('span', class_='reference-accessdate')\n",
    "        if access:\n",
    "            # e.g. \". Retrieved 24 February 2022\"\n",
    "            # strip leading punctuation/words\n",
    "            retrieved = access.get_text(separator=' ', strip=True)\n",
    "            # remove any leading period or 'Retrieved'\n",
    "            retrieved = retrieved.replace('Retrieved', '').lstrip('. ').strip()\n",
    "        else:\n",
    "            retrieved = None\n",
    "\n",
    "        # 3) first external URL in this citation\n",
    "        a = li.find('a', class_='external text')\n",
    "        url = a['href'] if a and a.has_attr('href') else None\n",
    "\n",
    "        results.append({\n",
    "            'citation_number': idx,\n",
    "            'text': text,\n",
    "            'retrieval_date': retrieved,\n",
    "            'url': url\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = glob.glob('../data/*/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_citations = []\n",
    "for citation in citations:\n",
    "    html = open(citation).read()\n",
    "    citations = parse_citations(html)\n",
    "    all_citations.append({\n",
    "        'citation_file': citation,\n",
    "        'citations': citations\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "citations_df = pd.DataFrame(all_citations)\n",
    "citations_df = (\n",
    "    citations_df\n",
    "        .assign(num_citations=citations_df['citations'].str.len())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (citations_df\n",
    "#  .explode('citations')\n",
    "#  .assign(\n",
    "#     citation_number=lambda x: x['citations'].apply(lambda x: x['citation_number']),\n",
    "#     text=lambda x: x['citations'].apply(lambda x: x['text']),\n",
    "#     retrieval_date=lambda x: x['citations'].apply(lambda x: x['retrieval_date']),\n",
    "#     url=lambda x: x['citations'].apply(lambda x: x['url'])\n",
    "#  )\n",
    "#  .drop(columns=['citations'])\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_df.to_json('../data/all-wikipedia-citations-parsed.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import glob\n",
    "deep_research_files = glob.glob('/Users/spangher/Downloads/drive-download-20250607T210740Z-1-001/*')\n",
    "all_full_texts = []\n",
    "for doc_file in deep_research_files:\n",
    "    doc = Document(doc_file)\n",
    "    full_text = '\\n\\n'.join(list(map(lambda x: x.text, doc.paragraphs)))\n",
    "    all_full_texts.append({\n",
    "        'file': doc_file,\n",
    "        'full_text': full_text\n",
    "    })\n",
    "\n",
    "\n",
    "all_full_texts_df = pd.DataFrame(all_full_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_full_texts_df = all_full_texts_df.assign(file=lambda x: x['file'].apply(lambda x: x.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_foldername = {\n",
    "'Musk, Trump Feud Overview_.docx': 'musk-trump-feud',\n",
    "'Russia-Ukraine Conflict Overview, 2022+_.docx': 'russia-ukraine-invasion',\n",
    "'COP29 Conference Overview Requested_.docx': 'cop29',\n",
    "'Panda Diplomacy_ Origins and Impacts_.docx': 'panda-diplomacy',\n",
    "'Australia_s Carbon Tax History_.docx': 'australia-carbon-tax'\n",
    "}\n",
    "all_full_texts_df = all_full_texts_df.assign(foldername=lambda df: df['file'].map(name_to_foldername)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in all_full_texts_df.iterrows():\n",
    "    with open(f\"../data/{row['foldername']}/deep-research/{row['file'].replace('.docx', '.txt')}\", 'w') as f:\n",
    "        f.write(row['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_full_texts_df\n",
    " .assign(file=lambda df: df['file'].str.replace('.docx', '.txt'))\n",
    " .assign(filepath=lambda df: df['foldername'] + '/deep-research/' + df['file'])\n",
    " .to_csv('../data/all-deep-research-full-texts.csv', index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(all_full_texts_df.loc[5]['full_text'].replace('\\n', '\\\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DEEP_RESEARCH_ARTICLE_TEXT_PROMPT = \"\"\"You are a helpful assistant. \n",
    "Here is raw text from Deep Research I generated.\n",
    "Please extract the title, content and thought process (which should be at the end).\n",
    "In the content field, pay special attention to keep ALL citation markers (e.g. 1, 2, 3, etc.), copy them exactly as they are. \n",
    "The thought process is the chain-of-thought reasoning that occurs at the end of the article, after the citations.\n",
    "\n",
    "Return in a JSON format with the following fields: title, content, citations, thought_process.\n",
    "Citations should be a list of responses.\n",
    "\n",
    "<article>\n",
    "{raw_article_text}\n",
    "</article>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class DeepResearchCitation(BaseModel):\n",
    "    citation_number: int\n",
    "    text: str\n",
    "    retrieval_date: str\n",
    "    url: str\n",
    "\n",
    "class DeepResearchArticle(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    thought_process: str\n",
    "    citations: list[DeepResearchCitation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CLEAN_DEEP_RESEARCH_ARTICLE_TEXT_PROMPT.format(raw_article_text=all_full_texts_df['full_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_research = prompt_openai(prompt, DeepResearchArticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_content = '\\n\\n'.join(all_full_texts_df.loc[3]['full_text'].split('\\n\\n')[1:]).split('Works cited')[0]\n",
    "pyperclip.copy(main_content.replace('\\n', '\\\\n').replace('\"', '\\\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f999dc8ea5a4eb0ab7be01630a4fe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Careful, does not reliably work and needs manual checking...\n",
    "\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "for _, row in tqdm(all_full_texts_df.iloc[1:].iterrows(), total=len(all_full_texts_df) - 1):\n",
    "    prompt = CLEAN_DEEP_RESEARCH_ARTICLE_TEXT_PROMPT.format(raw_article_text=row['full_text'])\n",
    "    deep_research = prompt_openai(prompt, DeepResearchArticle)\n",
    "    with open(f'../data/{row[\"foldername\"]}/deep-research/{row[\"foldername\"]}.json', 'w') as f:\n",
    "        json.dump(json.loads(deep_research.model_dump_json()), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>full_text</th>\n",
       "      <th>foldername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musk, Trump Feud Overview_.docx</td>\n",
       "      <td>A Collision of Titans: The Turbulent Saga of E...</td>\n",
       "      <td>musk-trump-feud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russia-Ukraine Conflict Overview, 2022+_.docx</td>\n",
       "      <td>The Russia-Ukraine Conflict: 2022 Onwards – Ge...</td>\n",
       "      <td>russia-ukraine-invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COP29 Conference Overview Requested_.docx</td>\n",
       "      <td>COP29 in Baku: Navigating Finance, Finalizing ...</td>\n",
       "      <td>cop29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Panda Diplomacy_ Origins and Impacts_.docx</td>\n",
       "      <td>The Enduring Allure and Strategic Significance...</td>\n",
       "      <td>panda-diplomacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia_s Carbon Tax History_.docx</td>\n",
       "      <td>Australia's Enduring Pursuit of Carbon Pricing...</td>\n",
       "      <td>australia-carbon-tax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file  \\\n",
       "0                Musk, Trump Feud Overview_.docx   \n",
       "2  Russia-Ukraine Conflict Overview, 2022+_.docx   \n",
       "3      COP29 Conference Overview Requested_.docx   \n",
       "4     Panda Diplomacy_ Origins and Impacts_.docx   \n",
       "5           Australia_s Carbon Tax History_.docx   \n",
       "\n",
       "                                           full_text               foldername  \n",
       "0  A Collision of Titans: The Turbulent Saga of E...          musk-trump-feud  \n",
       "2  The Russia-Ukraine Conflict: 2022 Onwards – Ge...  russia-ukraine-invasion  \n",
       "3  COP29 in Baku: Navigating Finance, Finalizing ...                    cop29  \n",
       "4  The Enduring Allure and Strategic Significance...          panda-diplomacy  \n",
       "5  Australia's Enduring Pursuit of Carbon Pricing...     australia-carbon-tax  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_full_texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

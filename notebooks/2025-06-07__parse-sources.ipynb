{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = open(os.path.expanduser('~/.openai-bloomberg-project-key.txt')).read().strip()\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_openai(prompt: str, response_format: BaseModel = None):\n",
    "    if response_format:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            response_format=response_format\n",
    "        )    \n",
    "        return response.choices[0].message.parsed\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_NEWS_SOURCES = \"\"\"\n",
    "You are a helpful news assistant. Here is a news article:\n",
    "\n",
    "<article>\n",
    "{news_article}\n",
    "</article>\n",
    "\n",
    "Please summarize each informational source providing information in the article. \n",
    "Include unnamed or passively expressed sources (e.g. \"witnesses\", \"price signals\") if there is information attributable to them.\n",
    "Include any facts that might have come from the source.\n",
    "Make sure each source you return refer to just one source. For example: if \"John and Jane\" both contribute the same information, generate two separate summaries, one for \"John\" and one for \"Jane\". \n",
    "Generate only ONE summary per source.\n",
    "\n",
    "For each source, provide the following information:\n",
    "    (1) Name: just the name of the source.\n",
    "    (2) Biography: A brief biography of the source mentioned in the article.\n",
    "    (3) Information: Restate the facts provided by the source. Be as SPECIFIC and be as VERBOSE as possible. \n",
    "        Contextualize ALL the information the source describes. State the full names of all people, places, events and ideas mentioned and\n",
    "        everything the source says with AS MUCH BACKGROUND INFORMATION from the article so I can fully understand the information\n",
    "        the source is giving. I will look at each source independently without looking at any others, so help me understand the context.\n",
    "\n",
    "Here are some examples:\n",
    "example 1:\n",
    "{{ \"Name\": \"Supermarkets around the country\",\n",
    "   \"Biography\": \"Retail stores that sell food and other household items\",\n",
    "   \"Information\": \"Supermarkets around the country alerted shoppers that prices are likely to continue going up due to the avian flu outbreak, with eggs now average $2.88 per dozen, up 52% since the first confirmed case of avian influenza in February.\"\n",
    "}}\n",
    "\n",
    "example 2:\n",
    "{{\n",
    "  \"Name\": \"The article's author (unnamed)\",\n",
    "  \"Biography\": \"The author of the article\",\n",
    "  \"Information\": \"The author stated that Wing, which is collaborating with FedEx and Walgreens on drone delivery, was the first to receive a limited Part 135 certificate. Wing is launching operations in Virginia this month, and the Standard certification allows UPS to send an unlimited number of drones to the skies, for their cargo load to exceed 55 pounds and for them to fly at night.\"\n",
    "}}\n",
    "\n",
    "example 3:\n",
    "{{\n",
    "   \"Name\": \"Delta's customers\",\n",
    "   \"Biography\": \"People who travel with Delta Air Lines\",\n",
    "   \"Information\": \"Delta's customers suggested that they preferred more space on flights amid the COVID-19 pandemic, and they continue to tell Delta that more space provides more peace of mind.\"\n",
    "}}\n",
    "\n",
    "example 4:\n",
    "{{\n",
    "   \"Name\": \"European Union countries\",\n",
    "   \"Biography\": \"Countries that are part of the European Union\",\n",
    "   \"Information\": \"European Union countries are working on adopting copyright rules that allow news companies and publishers to negotiate payments with large tech companies like Facebook, Microsoft and Google that use their content on their platforms.\"\n",
    "}}\n",
    "\n",
    "Output the summary in a list of python dictionaries as in the examples. Don't say anything else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class NewsSource(BaseModel):\n",
    "    Name: str\n",
    "    Biography: str\n",
    "    Information: str\n",
    "\n",
    "class NewsSourceOutput(BaseModel):\n",
    "    sources: List[NewsSource]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "news_articles_df = pd.read_csv('../data/all-articles-parsed.csv')\n",
    "all_articles_on_disk = glob.glob('../data/**/*.txt', recursive=True)\n",
    "news_articles_df = news_articles_df.assign(path_name=\n",
    "    lambda df: df.apply(lambda x: list(filter(lambda y: x['filename'] in y and x['issue'] in y, all_articles_on_disk))[0], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_sources = []\n",
    "for _, row in tqdm(news_articles_df.iterrows(), total=len(news_articles_df)):\n",
    "    prompt = EXTRACT_NEWS_SOURCES.format(news_article=row['content'])\n",
    "    output = prompt_openai(prompt, NewsSourceOutput)\n",
    "    sources = json.loads(output.model_dump_json())['sources']\n",
    "    all_sources.append({\n",
    "        'url': row['url'],\n",
    "        'sources': sources\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources_df = pd.DataFrame(all_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>content</th>\n",
       "      <th>issue</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia kills off carbon tax</td>\n",
       "      <td>Lenore Taylor</td>\n",
       "      <td>https://www.theguardian.com/environment/2014/j...</td>\n",
       "      <td>Wed 16 Jul 2014 21.51 EDT</td>\n",
       "      <td>Australia’s carbon price has been repealed, le...</td>\n",
       "      <td>australia-carbon-tax</td>\n",
       "      <td>guardian.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carbon Pricing in Australia</td>\n",
       "      <td>Wikipedia Contributors</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Carbon_pricing_i...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A carbon pricing scheme in Australia was intro...</td>\n",
       "      <td>australia-carbon-tax</td>\n",
       "      <td>wikipedia.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title                  author  \\\n",
       "0  Australia kills off carbon tax           Lenore Taylor   \n",
       "1     Carbon Pricing in Australia  Wikipedia Contributors   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.theguardian.com/environment/2014/j...   \n",
       "1  https://en.wikipedia.org/wiki/Carbon_pricing_i...   \n",
       "\n",
       "            publication_date  \\\n",
       "0  Wed 16 Jul 2014 21.51 EDT   \n",
       "1                    Unknown   \n",
       "\n",
       "                                             content                 issue  \\\n",
       "0  Australia’s carbon price has been repealed, le...  australia-carbon-tax   \n",
       "1  A carbon pricing scheme in Australia was intro...  australia-carbon-tax   \n",
       "\n",
       "        filename  \n",
       "0   guardian.txt  \n",
       "1  wikipedia.txt  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_articles_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(news_articles_df\n",
    " [['title', 'author', 'content', 'issue', 'url']]\n",
    " .merge(all_sources_df)\n",
    " .to_json(\n",
    "     orient='records',\n",
    "     path_or_buf='../data/all-news-articles-with-parsed-sources.json', \n",
    "     lines=True\n",
    "    )\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize sources Wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "wiki_citations = list(jsonlines.open('../data/all-wikipedia-citations-parsed.json'))\n",
    "wiki_citation_df = pd.DataFrame(wiki_citations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = (\n",
    "    news_articles_df\n",
    "      .loc[lambda df: df['filename'] == 'wikipedia.txt']\n",
    " )\n",
    "\n",
    "test_article = wiki_articles.iloc[2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_citations_with_articles = (\n",
    "    wiki_citation_df\n",
    "        .assign(join_key=lambda df: df['citation_file'].str.split('/').str.get(2))\n",
    "        .merge(wiki_articles, left_on='join_key', right_on='issue')\n",
    "        .drop(columns=['join_key', 'filename', 'path_name', 'citation_file'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in wiki_citations_with_articles.iterrows():\n",
    "    row['content']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text, citations = row['content'], row['citations']\n",
    "paragraphs = article_text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_by_citation = defaultdict(list)\n",
    "for citation in citations:\n",
    "    for paragraph in paragraphs:\n",
    "        if f\"[{citation['citation_number']}]\" in paragraph:\n",
    "            paragraphs_by_citation[citation['citation_number']].append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    93\n",
       "2    10\n",
       "4     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(paragraphs_by_citation).str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The carbon price was part of a broad energy reform package called the Clean Energy Futures Plan, which aimed to reduce greenhouse gas emissions in Australia by 5 % below 2000 levels by 2020 and 80 % below 2000 levels by 2050. Although Australia does not levy a direct carbon price \\\\[5], the plan set out to achieve these targets by encouraging Australia’s largest emitters to increase energy efficiency and invest in sustainable energy. The scheme was administered by the Clean Energy Regulator. Compensation to industry and households was funded by the revenue derived from the charge. The scheme required entities which emit over 25,000 tonnes of carbon dioxide equivalent greenhouse gases per year, and which were not in the transport or agriculture sectors, to obtain emissions permits, called carbon units. Carbon units were either purchased from the government or issued free as part of industry assistance measures. As part of the scheme, personal income tax was reduced for those earning less than A\\\\$80,000 per year and the tax-free threshold was increased from A\\\\$6,000 to A\\\\$18,200 \\\\[6]. Initially the price of a permit for one tonne of carbon was fixed at A\\\\$23 for the 2012–13 financial year, with unlimited permits being available from the government. The fixed price rose to A\\\\$24.15 for 2013–14.']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_by_citation[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_WIKIPEDIA_SOURCES = \"\"\"\n",
    "You are a helpful writing assistant. I will give you a Wikipedia article and {k} citations from that article:\n",
    "\n",
    "<article>\n",
    "{wikipedia_article}\n",
    "</article>\n",
    "\n",
    "<citations>\n",
    "{citations}\n",
    "</citations>\n",
    "\n",
    "For each citation, please search for [citation_id] in the article and summarize the information provided by the the citation in the article. \n",
    "Look for every time that citation is referenced in the article, by searching for [citation_id] in the article.\n",
    "Summarize all the information provided by the citation in the article.\n",
    "Generate only ONE summary per citation.\n",
    "\n",
    "For each citation, provide the following information:\n",
    "    (1) citation_id: The id of the citation.\n",
    "    (2) Information: Restate the facts provided by the citation. Be as SPECIFIC and be as VERBOSE as possible. \n",
    "        Contextualize ALL the information the citation describes. State the full names of all people, places, events and ideas mentioned and\n",
    "        everything the citation says with AS MUCH BACKGROUND INFORMATION from the article so I can fully understand the information\n",
    "        the citation is giving. I will look at each citation independently without looking at any others, so help me understand the context.\n",
    "Output the summaries in a list of python dictionaries as in the examples. Don't say anything else.\n",
    "\n",
    "Here are some examples:\n",
    "example 1:\n",
    "{{ \"citation_id\": \"1\",\n",
    "   \"Information\": \"Supermarkets around the country alerted shoppers that prices are likely to continue going up due to the avian flu outbreak, with eggs now average $2.88 per dozen, up 52% since the first confirmed case of avian influenza in February.\"\n",
    "}}\n",
    "\n",
    "example 2:\n",
    "{{\n",
    "  \"citation_id\": \"5\",\n",
    "  \"Information\": \"The author stated that Wing, which is collaborating with FedEx and Walgreens on drone delivery, was the first to receive a limited Part 135 certificate. Wing is launching operations in Virginia this month, and the Standard certification allows UPS to send an unlimited number of drones to the skies, for their cargo load to exceed 55 pounds and for them to fly at night.\"\n",
    "}}\n",
    "\n",
    "example 3:\n",
    "{{\n",
    "   \"citation_id\": \"2\",\n",
    "   \"Information\": \"Delta's customers suggested that they preferred more space on flights amid the COVID-19 pandemic, and they continue to tell Delta that more space provides more peace of mind.\"\n",
    "}}\n",
    "\n",
    "example 4:\n",
    "{{\n",
    "   \"citation_id\": \"20\",\n",
    "   \"Information\": \"European Union countries are working on adopting copyright rules that allow news companies and publishers to negotiate payments with large tech companies like Facebook, Microsoft and Google that use their content on their platforms.\"\n",
    "}}\n",
    "\n",
    "Output the summaries in a list of python dictionaries as in the examples. Don't say anything else.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, create_model\n",
    "from typing import Annotated\n",
    "from annotated_types import Len\n",
    "\n",
    "class WikipediaCitation(BaseModel):\n",
    "    citation_id: str\n",
    "    Information: str\n",
    "\n",
    "def make_wikipedia_citation_structure(min_len: int, max_len: int):\n",
    "    return create_model(\n",
    "        'WikipediaCitationOutput',\n",
    "        citations=(Annotated[list[WikipediaCitation], Len(min_length=min_len, max_length=max_len)], ...),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_citation(citation):    \n",
    "    return f\"\"\"[{citation['citation_number']}] {citation['text']}, url: {citation['url']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_list(input_list, batch_size):\n",
    "    \"\"\"Yield successive n-sized chunks from input_list.\"\"\"\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        yield input_list[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wiki_article_with_citations(article_text, citations, start_idx=0, max_citations=15):\n",
    "    paragraphs = article_text.split('\\n\\n')\n",
    "    paragraphs_with_citations, found_citations = [],[]\n",
    "    for idx, citation in enumerate(citations[start_idx:]):\n",
    "        found = False\n",
    "        for paragraph in paragraphs:\n",
    "            if f\"[{citation['citation_number']}]\" in paragraph:\n",
    "                paragraphs_with_citations.append(paragraph)\n",
    "                found = True\n",
    "        if found:\n",
    "            found_citations.append(citation)\n",
    "            if len(found_citations) >= max_citations:\n",
    "                break\n",
    "    return '...'.join(paragraphs_with_citations), found_citations, (idx + start_idx) == len(citations) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7526919499014b1298c91d0286fb2409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 (29) of 709  citations...\n",
      "Found 15 (45) of 709  citations...\n",
      "Found 15 (61) of 709  citations...\n",
      "Found 15 (77) of 709  citations...\n",
      "Found 15 (93) of 709  citations...\n",
      "Found 15 (109) of 709  citations...\n",
      "Found 15 (125) of 709  citations...\n",
      "Found 15 (141) of 709  citations...\n",
      "Found 15 (157) of 709  citations...\n",
      "Found 15 (173) of 709  citations...\n",
      "Found 15 (189) of 709  citations...\n",
      "Found 15 (205) of 709  citations...\n",
      "Found 15 (221) of 709  citations...\n",
      "Found 15 (237) of 709  citations...\n",
      "Found 15 (253) of 709  citations...\n",
      "Found 15 (269) of 709  citations...\n",
      "Found 15 (285) of 709  citations...\n",
      "Found 15 (301) of 709  citations...\n",
      "Found 15 (317) of 709  citations...\n",
      "Found 15 (333) of 709  citations...\n",
      "Found 15 (349) of 709  citations...\n",
      "Found 15 (365) of 709  citations...\n",
      "Found 15 (381) of 709  citations...\n",
      "Found 15 (397) of 709  citations...\n",
      "Found 15 (413) of 709  citations...\n",
      "Found 15 (429) of 709  citations...\n",
      "Found 15 (445) of 709  citations...\n",
      "Found 15 (461) of 709  citations...\n",
      "Found 15 (477) of 709  citations...\n",
      "Found 15 (493) of 709  citations...\n",
      "Found 15 (509) of 709  citations...\n",
      "Found 15 (525) of 709  citations...\n",
      "Found 15 (545) of 709  citations...\n",
      "Found 15 (565) of 709  citations...\n",
      "Found 15 (581) of 709  citations...\n",
      "Found 15 (597) of 709  citations...\n",
      "Found 15 (613) of 709  citations...\n",
      "Found 15 (630) of 709  citations...\n",
      "Found 15 (646) of 709  citations...\n",
      "Found 15 (662) of 709  citations...\n",
      "Found 15 (678) of 709  citations...\n",
      "Found 15 (694) of 709  citations...\n",
      "Found 14 (709) of 709  citations...\n"
     ]
    }
   ],
   "source": [
    "all_citation_summaries = []\n",
    "batch_size = 500\n",
    "TREAT_LONG_ARTICLES = True\n",
    "\n",
    "long_wiki_articles = wiki_citations_with_articles.loc[lambda df: df['num_citations'] >= 200]\n",
    "for _, row in tqdm(long_wiki_articles.iterrows(), total=len(long_wiki_articles)):\n",
    "    citations = row['citations']\n",
    "    citation_summaries = []\n",
    "    batches = list(batch_list(citations, batch_size))\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        paragraphs_with_citations, found_citations, all_citations_found = filter_wiki_article_with_citations(\n",
    "            row['content'], citations, start_idx=start_idx, max_citations=15\n",
    "        )\n",
    "        print(f\"Found {len(found_citations)} ({found_citations[-1]['citation_number']}) of {len(citations)}  citations...\")\n",
    "        citations_formatted = [format_citation(citation) for citation in found_citations]\n",
    "        if TREAT_LONG_ARTICLES:\n",
    "            prompt = AUGMENT_WIKIPEDIA_SOURCES.format(\n",
    "                k=len(citations_formatted),\n",
    "                wikipedia_article=paragraphs_with_citations,\n",
    "                citations='\\n'.join(citations_formatted)\n",
    "            )\n",
    "        else:\n",
    "            prompt = AUGMENT_WIKIPEDIA_SOURCES.format(\n",
    "                k=len(citations_formatted),\n",
    "                wikipedia_article=row['content'],\n",
    "                citations='\\n'.join(citations_formatted)\n",
    "            )\n",
    "        output = prompt_openai(\n",
    "            prompt, \n",
    "            make_wikipedia_citation_structure(len(citations_formatted), len(citations_formatted))\n",
    "        )\n",
    "        citation_summaries.extend(list(map(lambda x: json.loads(x.model_dump_json()), output.citations)))\n",
    "        start_idx = found_citations[-1]['citation_number'] + 1\n",
    "        if all_citations_found:\n",
    "            break\n",
    "\n",
    "    all_citation_summaries.append({\n",
    "        'issue': row['issue'],\n",
    "        'citation_summaries': citation_summaries\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_citation_summaries).to_json(\n",
    "    '../data/all-wikipedia-citations-parsed-summarized.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Research Citations Summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_DEEP_RESEARCH_SOURCES = \"\"\"\n",
    "You are a helpful writing assistant. I will give you a high-level summary of the research report, and then\n",
    "I'll give you snippets from a research report and {k} citations from the report. The snippets will specifically contain sentences pertaining to the citations.\n",
    "\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "<article_snippets>\n",
    "{deep_research_article_snippets}\n",
    "</article_snippets>\n",
    "\n",
    "<citations>\n",
    "{citations}\n",
    "</citations>\n",
    "\n",
    "For each citation, please search for citation_id in the article and summarize the information provided by the the citation in the article. \n",
    "The citation will referenced with a number at the end of sentences (e.g. \"The treaty was signed yesterday.2\").\n",
    "Summarize all the information provided by the citation in the article.\n",
    "Generate only ONE summary per citation.\n",
    "\n",
    "For each citation, provide the following information:\n",
    "    (1) citation_id: The id of the citation.\n",
    "    (2) Information: Restate the facts provided by the citation. Be as SPECIFIC and be as VERBOSE as possible. \n",
    "        Contextualize ALL the information the citation describes. State the full names of all people, places, events and ideas mentioned and\n",
    "        everything the citation says with AS MUCH BACKGROUND INFORMATION from the article so I can fully understand the information\n",
    "        the citation is giving. I will look at each citation independently without looking at any others, so help me understand the context.\n",
    "Output the summaries in a list of python dictionaries as in the examples. Don't say anything else.\n",
    "\n",
    "Here are some examples:\n",
    "example 1:\n",
    "{{ \"citation_id\": \"1\",\n",
    "   \"Information\": \"Supermarkets around the country alerted shoppers that prices are likely to continue going up due to the avian flu outbreak, with eggs now average $2.88 per dozen, up 52% since the first confirmed case of avian influenza in February.\"\n",
    "}}\n",
    "\n",
    "example 2:\n",
    "{{\n",
    "  \"citation_id\": \"5\",\n",
    "  \"Information\": \"The author stated that Wing, which is collaborating with FedEx and Walgreens on drone delivery, was the first to receive a limited Part 135 certificate. Wing is launching operations in Virginia this month, and the Standard certification allows UPS to send an unlimited number of drones to the skies, for their cargo load to exceed 55 pounds and for them to fly at night.\"\n",
    "}}\n",
    "\n",
    "example 3:\n",
    "{{\n",
    "   \"citation_id\": \"2\",\n",
    "   \"Information\": \"Delta's customers suggested that they preferred more space on flights amid the COVID-19 pandemic, and they continue to tell Delta that more space provides more peace of mind.\"\n",
    "}}\n",
    "\n",
    "example 4:\n",
    "{{\n",
    "   \"citation_id\": \"20\",\n",
    "   \"Information\": \"European Union countries are working on adopting copyright rules that allow news companies and publishers to negotiate payments with large tech companies like Facebook, Microsoft and Google that use their content on their platforms.\"\n",
    "}}\n",
    "\n",
    "Output the summaries in a list of python dictionaries as in the examples. Don't say anything else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"\n",
    "Please summarize this article in 1-2 sentences. Return just the summary, no other text.\n",
    "\n",
    "<article>\n",
    "{article}\n",
    "</article>\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, create_model\n",
    "from typing import Annotated\n",
    "from annotated_types import Len\n",
    "\n",
    "class DeepResearchCitation(BaseModel):\n",
    "    citation_id: str\n",
    "    Information: str\n",
    "\n",
    "def make_deep_research_citation_structure(min_len: int, max_len: int):\n",
    "    return create_model(\n",
    "        'DeepResearchCitationOutput',\n",
    "        citations=(Annotated[list[DeepResearchCitation], Len(min_length=min_len, max_length=max_len)], ...),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_research_parsed = glob.glob('../data/*/deep-research/*.json')\n",
    "all_deep_research_parsed = []\n",
    "for f in deep_research_parsed:\n",
    "    with open(f, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_deep_research_parsed.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "all_deep_research_parsed_df = (\n",
    "    pd.DataFrame(all_deep_research_parsed)\n",
    "        .assign(content=lambda df: df.apply(lambda x: x['content'] if pd.isnull(x['full_content']) else x['full_content'], axis=1))\n",
    "        .drop(columns=['full_content'])\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deep_research_parsed_df.to_json('../data/all-deep-research-content.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_deep_research_article_with_citations(article_text, citations, start_idx=0, max_citations=15):\n",
    "    paragraphs = article_text.split('\\n\\n')\n",
    "    paragraphs_with_citations, found_citations = [],[]\n",
    "    for idx, citation in enumerate(citations[start_idx:]):\n",
    "        found = False\n",
    "        for paragraph in paragraphs:\n",
    "            if f\".{citation['citation_number']} \" in paragraph:\n",
    "                paragraphs_with_citations.append(paragraph)\n",
    "                found = True\n",
    "        if found:\n",
    "            found_citations.append(citation)\n",
    "            if len(found_citations) >= max_citations:\n",
    "                break\n",
    "    return '...'.join(paragraphs_with_citations), found_citations, (idx + start_idx) == len(citations) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     6561\n",
       "1     3310\n",
       "2     6767\n",
       "3    12518\n",
       "4     6416\n",
       "Name: content, dtype: int64"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_deep_research_parsed_df['content'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'citation_number': 1,\n",
       "  'text': 'Net Zero - DCCEEW',\n",
       "  'retrieval_date': 'June 6, 2025',\n",
       "  'url': 'https://www.dcceew.gov.au/climate-change/emissions-reduction/net-zero'},\n",
       " {'citation_number': 2,\n",
       "  'text': 'Safeguard Mechanism - Clean Energy Regulator',\n",
       "  'retrieval_date': 'June 6, 2025',\n",
       "  'url': 'https://cer.gov.au/schemes/safeguard-mechanism'}]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_deep_research_parsed_df['citations'].iloc[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22\n",
       "1    10\n",
       "2    24\n",
       "3     5\n",
       "4    23\n",
       "Name: citations, dtype: int64"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_deep_research_parsed_df['citations'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee046c370fa40db845b315260c6ce6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 (3) of 22  citations...\n",
      "Found 3 (8) of 22  citations...\n",
      "Found 3 (13) of 22  citations...\n",
      "Found 3 (17) of 22  citations...\n",
      "Found 2 (20) of 22  citations...\n"
     ]
    }
   ],
   "source": [
    "# all_deep_research_citation_summaries = []\n",
    "batch_size = 500\n",
    "TREAT_LONG_ARTICLES = True\n",
    "\n",
    "for _, row in tqdm(\n",
    "    all_deep_research_parsed_df.iloc[:1].iterrows(), \n",
    "    total=len(all_deep_research_parsed_df)\n",
    "):\n",
    "    citations = row['citations']\n",
    "    citation_summaries = []\n",
    "    batches = list(batch_list(citations, batch_size))\n",
    "    summary = prompt_openai(SUMMARY_PROMPT.format(article=row['content']))\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        paragraphs_with_citations, found_citations, all_citations_found = (\n",
    "            filter_deep_research_article_with_citations(\n",
    "                row['content'], citations, start_idx=start_idx, max_citations=3\n",
    "            )\n",
    "        )\n",
    "        print(f\"Found {len(found_citations)} ({found_citations[-1]['citation_number']}) of {len(citations)}  citations...\")\n",
    "        citations_formatted = [format_citation(citation) for citation in found_citations]\n",
    "        prompt = AUGMENT_DEEP_RESEARCH_SOURCES.format(\n",
    "            summary=summary,\n",
    "            k=len(citations_formatted),\n",
    "            deep_research_article_snippets=paragraphs_with_citations,\n",
    "            citations='\\n'.join(citations_formatted)\n",
    "        )\n",
    "        output = prompt_openai(\n",
    "            prompt, \n",
    "            make_deep_research_citation_structure(len(citations_formatted), len(citations_formatted))\n",
    "        )\n",
    "        citation_summaries.extend(list(map(lambda x: json.loads(x.model_dump_json()), output.citations)))\n",
    "        start_idx = found_citations[-1]['citation_number'] + 1\n",
    "        if all_citations_found:\n",
    "            break\n",
    "\n",
    "    all_deep_research_citation_summaries.append({\n",
    "        'issue': row['title'],\n",
    "        'citation_summaries': citation_summaries\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_deep_research_citation_summaries).to_json('../data/deep-research-citation-summaries.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(json.dumps(citation_summaries[:3], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Australia's carbon pricing journey has been marked by policy shifts from a direct carbon tax under the Clean Energy Act 2011, which was repealed in 2014, to the 'Direct Action' policy with the Emissions Reduction Fund, culminating in the reformed Safeguard Mechanism from July 2023 that now acts as a baseline-and-credit emissions trading scheme aimed at reducing emissions from the country's largest industrial emitters. This evolution reflects ongoing efforts to balance environmental goals with economic and political challenges while aiming for a 43% reduction in emissions below 2005 levels by 2030 and net-zero emissions by 2050.\""
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyperclip.copy(summary)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'url': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               Australia's key climate policy faces an uncert...\n",
       "author                                                     Tom Lowrey\n",
       "url                 https://www.abc.net.au/news/2025-03-03/safegua...\n",
       "publication_date                                       Sun 2 Mar 2025\n",
       "content             Some business and climate groups have voiced t...\n",
       "issue                                            australia-carbon-tax\n",
       "filename                                                      abc.txt\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_articles.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../data/australia-carbon-tax/news/abc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
